{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8604684,"sourceType":"datasetVersion","datasetId":5148697}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=184056641\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=183010840\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=182786979\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=182712552\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=182257992\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"Details of Steps can be found on Google Slides via:Â¶\n\nhttps://docs.google.com/presentation/d/1sb3QkXiYooHqi3p-tkGVUqwqFKd-601_pzU96W1drw0/edit?usp=sharing","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nwandb_key_label = \"WANDB_KEY\"\nwandb_key= UserSecretsClient().get_secret(wandb_key_label)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T19:56:36.606687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import git\ngit.Repo.clone_from('https://github.com/Lumin-Lab/BerCuration', '/kaggle/working/scarf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r /kaggle/working/scarf/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef save_csv_file(df, path):\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n    df.to_csv(path, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data_path = \"/kaggle/input/ber-stratified-samples/BER_stratified_sample.csv\"\nn_splits = 5\noutput_dir = \"/kaggle/working/output\"\nconfig_dir=\"/kaggle/working/scarf/configs\"\nscarf_model_name = \"scarf\"\nmlp_model_name = \"mlp\"\nscarf_batch_size = 32\n# scarf_epochs = 25 \nscarf_epochs = 1 \nscarf_lr = 3e-5\nscarf_emb_dim = 32\nscarf_encoder_depth = 3\nscarf_corruption_rate=0.3\n# max_depth = 6\nmax_depth = 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nX_raw = pd.read_csv(raw_data_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport os\nkf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\nfor i, (train_index, test_index) in enumerate(kf.split(X_raw)):\n    save_csv_file(X_raw.iloc[train_index], f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    save_csv_file(X_raw.iloc[test_index], f\"{output_dir}/split_{i+1}/raw_test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"input: data_path={raw_data_path}\" ,\noutput:{output_dir}/processed.csv \"\"\"\n\ncommand = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}\" \\\n      --data_path={raw_data_path} \\\n      --output_csv_name \"processed\" \\\n      --is_train\n    \"\"\"\nos.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_to_copy = [\"column_type_classification.yaml\", \"encoder.joblib\", \"scaler.joblib\", \"train_stats.json\"]\nfor i in range(n_splits):\n    for file_name in files_to_copy:\n        src = f\"{output_dir}/{file_name}\"\n        dest = f\"{output_dir}/split_{i+1}/{file_name}\"\n        os.system(f\"cp {src} {dest}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(n_splits):\n    \"\"\"input: data_path \"{output_dir}/split_{i+1}/raw_train.csv\" ,\n    output:{output_dir}/split_{i+1}/processed_train.csv \"\"\"\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"{config_dir}\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/raw_train.csv\" \\\n      --output_csv_name \"processed_train\" \\\n    \"\"\"\n    os.system(command)\n    \"\"\"input: data_path \"{output_dir}/split_{i+1}/raw_test.csv\" ,\n    output:{output_dir}/split_{i+1}/processed_test.csv \"\"\"\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"{config_dir}\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/raw_test.csv\" \\\n      --output_csv_name \"processed_test\"\n    \"\"\"\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the SCARF Encoder on the Train Dataset**","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login(key=wandb_key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\"\"\"\nInput: train_data_path=\"{output_dir}/split_{i+1}/processed_train.csv\"\n \nOutput: /kaggle/working/output/split_{split}/scarf.pt  \n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_scarf.py\\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --train_data_path=\"{output_dir}/split_{i+1}/processed_train.csv\"\\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name=\"{scarf_model_name}\" \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --wandb_project_name='SCARF_Project' \\\n      --wandb_entity='urbancomp' \n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtain the SCARF embeddings for different train splits, and save the result**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nInput: data_path=\"{output_dir}/split_{i+1}/processed_train.csv\"\n \nOutput: {output_dir}/split_{i+1}/train.npy\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --data_path=\"{output_dir}/split_{i+1}/processed_train.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"train\"\n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtain the SCARF embeddings for different test splits, and save the result**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nInput: data_path=\"{output_dir}/split_{i+1}/processed_test.csv\"\n \nOutput: {output_dir}/split_{i+1}/test.npy\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py\\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --data_path=\"{output_dir}/split_{i+1}/processed_test.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"test\"\n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nInput: data_path=\"{output_dir}/processedt.csv\"\n \nOutput: {output_dir}/split_1/all_emb.npy\n\"\"\"\n\ncommand = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_1\" \\\n      --data_path=\"{output_dir}/processed.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"all_emb\"\n    \"\"\"\n\nos.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\ndef get_rf(max_depth=max_depth):\n    return RandomForestClassifier(max_depth=max_depth, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Original Data**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nprocessed = pd.read_csv(f\"{output_dir}/processed.csv\")\nall_emb = np.load(f\"{output_dir}/split_1/all_emb.npy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import cross_val_predict\nfrom cleanlab import Datalab\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_processed = processed.drop(columns=[\"EnergyRating\"])\nlabels = processed['EnergyRating']\nclf = get_rf()\npred_probs = cross_val_predict(\n    clf,\n    X_processed,\n    labels,\n    cv=kf,\n    method=\"predict_proba\",\n)\nKNN = NearestNeighbors(metric='euclidean')\nKNN.fit(X_processed.values)\n\nknn_graph = KNN.kneighbors_graph(mode=\"distance\")\ndata = {\"X\": X_processed.values, \"y\": labels}\n\nlab = Datalab(data, label_name=\"y\")\nlab.find_issues(pred_probs=pred_probs, knn_graph=knn_graph)\n\ndisplay(lab.report())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Issues\nlabel_issues_num =  54404\noutlier_nums = 2296\nduplicates_nums = 8785\n\nissue_results = lab.get_issues(\"label\")\nsorted_issues = issue_results.sort_values(\"label_score\").index\n\nsorted_issues_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_issues, \"Unnamed: 0\"]\n})\nsave_csv_file(sorted_issues_df[:label_issues_num], f\"{output_dir}/random_forest/data/label_issues.csv\")\nprint(len(sorted_issues_df))\n\nprint(\"Label Issues\")\ndisplay(X_raw.iloc[sorted_issues].assign(\n    given_label=labels.iloc[sorted_issues],\n    predicted_label=issue_results[\"predicted_label\"].iloc[sorted_issues]\n).head())\n\n# Outliers\noutlier_results = lab.get_issues(\"outlier\")\nsorted_outliers= outlier_results.sort_values(\"outlier_score\").index\nprint(\"Outliers\")\ndisplay(X_raw.iloc[sorted_outliers].head())\n\nsorted_outliers_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_outliers[:outlier_nums], \"Unnamed: 0\"]\n})\nprint(len(sorted_outliers_df))\nsave_csv_file(sorted_outliers_df, f\"{output_dir}/random_forest/data/outliers.csv\")\n\n\n# Near-duplicate issues\nduplicate_results = lab.get_issues(\"near_duplicate\")\nsorted_duplicates = duplicate_results.sort_values(\"near_duplicate_score\").index\nprint(\"Near-duplicate issues\")\ndisplay(duplicate_results.sort_values(\"near_duplicate_score\").head())\nsorted_duplicates_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_duplicates, \"Unnamed: 0\"]\n})\nprint(len(sorted_duplicates_df))\nsave_csv_file(sorted_duplicates_df[:duplicates_nums], f\"{output_dir}/random_forest/data/duplicates.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before Removal**","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_rf()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    clf.fit(train_df.drop(columns=[\"EnergyRating\"]), train_df[\"EnergyRating\"])\n    acc = clf.score(test_df.drop(columns=[\"EnergyRating\"]), test_df[\"EnergyRating\"])\n    predict = clf.predict(test_df.drop(columns=[\"EnergyRating\"]))\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/random_forest/data/results_after_removal.csv\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After Removal","metadata":{}},{"cell_type":"code","source":"for i in range(n_splits):\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    label_issue_index = pd.read_csv(f\"{output_dir}/random_forest/data/label_issues.csv\")\n    train_df_removed = train_df[~train_df[\"Unnamed: 0\"].isin(label_issue_index[\"Unnamed: 0\"])]\n    print(len(train_df_removed), len(train_df_removed)/len(train_df))\n    save_csv_file(train_df_removed, f\"{output_dir}/split_{i+1}/train_removed.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/train_removed.csv\" \\\n      --output_csv_name \"processed_train_removed\" \\\n    \"\"\"\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_rf()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train_removed.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    clf.fit(train_df.drop(columns=[\"EnergyRating\"]), train_df[\"EnergyRating\"])\n    acc = clf.score(test_df.drop(columns=[\"EnergyRating\"]), test_df[\"EnergyRating\"])\n    predict = clf.predict(test_df.drop(columns=[\"EnergyRating\"]))\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/random_forest/data/results_after_removal.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scarf Embedding**","metadata":{}},{"cell_type":"code","source":"\nX_processed = pd.DataFrame(all_emb, columns=[f\"feature_{i+1}\" for i in range(all_emb.shape[1])])\nlabels = processed['EnergyRating']\nclf = get_rf()\npred_probs = cross_val_predict(\n    clf,\n    X_processed,\n    labels,\n    cv=kf,\n    method=\"predict_proba\",\n)\nKNN = NearestNeighbors(metric='euclidean')\nKNN.fit(X_processed.values)\n\nknn_graph = KNN.kneighbors_graph(mode=\"distance\")\ndata = {\"X\": X_processed.values, \"y\": labels}\n\nlab = Datalab(data, label_name=\"y\")\nlab.find_issues(pred_probs=pred_probs, knn_graph=knn_graph)\ndisplay(lab.report())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Issues\nlabel_issues_num =  52164\noutlier_nums = 3752\nduplicates_nums = 7983\n\nissue_results = lab.get_issues(\"label\")\nsorted_issues = issue_results.sort_values(\"label_score\").index\n\nsorted_issues_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_issues, \"Unnamed: 0\"]\n})\nsave_csv_file(sorted_issues_df[:label_issues_num], f\"{output_dir}/random_forest/emb/label_issues.csv\")\nprint(len(sorted_issues_df))\n\nprint(\"Label Issues\")\ndisplay(X_raw.iloc[sorted_issues].assign(\n    given_label=labels.iloc[sorted_issues],\n    predicted_label=issue_results[\"predicted_label\"].iloc[sorted_issues]\n).head())\n\n# Outliers\noutlier_results = lab.get_issues(\"outlier\")\nsorted_outliers= outlier_results.sort_values(\"outlier_score\").index\nprint(\"Outliers\")\ndisplay(X_raw.iloc[sorted_outliers].head())\n\nsorted_outliers_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_outliers[:outlier_nums], \"Unnamed: 0\"]\n})\nprint(len(sorted_outliers_df))\nsave_csv_file(sorted_outliers_df, f\"{output_dir}/random_forest/emb/outliers.csv\")\n\n\n# Near-duplicate issues\nduplicate_results = lab.get_issues(\"near_duplicate\")\nsorted_duplicates = duplicate_results.sort_values(\"near_duplicate_score\").index\nprint(\"Near-duplicate issues\")\ndisplay(duplicate_results.sort_values(\"near_duplicate_score\").head())\nsorted_duplicates_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_duplicates, \"Unnamed: 0\"]\n})\nprint(len(sorted_duplicates_df))\nsave_csv_file(sorted_duplicates_df[:duplicates_nums], f\"{output_dir}/random_forest/emb/duplicates.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before Removal**","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_rf()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    train_X = np.load(f\"{output_dir}/split_{i+1}/train.npy\")\n    test_X = np.load(f\"{output_dir}/split_{i+1}/test.npy\")\n    clf.fit(train_X, train_df[\"EnergyRating\"])\n    acc = clf.score(test_X, test_df[\"EnergyRating\"],\n             )\n    predict = clf.predict(test_X)\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/random_forest/emb/results_before_removal.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After Removal**","metadata":{}},{"cell_type":"code","source":"original_rating_encoding = {\n    \"A1\": 0,\n    \"A2\": 1,\n    \"A3\": 2,\n    \"B1\": 3,\n    \"B2\": 4,\n    \"B3\": 5,\n    \"C1\": 6,\n    \"C2\": 7,\n    \"C3\": 8,\n    \"D1\": 9,\n    \"D2\": 10,\n    \"E1\": 11,\n    \"E2\": 12,\n    \"F\": 13,\n    \"G\": 14\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(n_splits):\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    train_X = np.load(f\"{output_dir}/split_{i+1}/train.npy\")\n    \n    label_issue_index = pd.read_csv(f\"{output_dir}/random_forest/emb/label_issues.csv\")\n    train_df_removed = train_df[~train_df[\"Unnamed: 0\"].isin(label_issue_index[\"Unnamed: 0\"])]\n    train_X_removed = train_X[train_df_removed.index]\n    train_df_removed['EnergyRating'] = train_df_removed['EnergyRating'].str.strip()\n    train_df_removed['EnergyRating'] = train_df_removed['EnergyRating'].map(original_rating_encoding)\n    print(len(train_df_removed), len(train_df_removed)/len(train_df))\n    save_csv_file(train_df_removed, f\"{output_dir}/split_{i+1}/train_removed.csv\")\n    np.save(f\"{output_dir}/split_{i+1}/train_removed.npy\", train_X_removed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_rf()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/train_removed.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    train_X = np.load(f\"{output_dir}/split_{i+1}//train_removed.npy\")\n    test_X = np.load(f\"{output_dir}/split_{i+1}/test.npy\")\n    clf.fit(train_X, train_df[\"EnergyRating\"])\n    acc = clf.score(test_X, test_df[\"EnergyRating\"],\n             )\n    predict = clf.predict(test_X)\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/random_forest/emb/results_after_removal.csv\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MLP**","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nhidden_layer_sizes = [256 , 128 , 64 , 32 , 16]\nmax_iter = 25\nbatch_size = 32\ndef get_mlp(hidden_layer_sizes=hidden_layer_sizes,\n           max_iter= max_iter,\n           batch_size = batch_size):\n    return MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n                        random_state= 42,\n                        max_iter= max_iter,\n                        batch_size = batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_processed = processed.drop(columns=[\"EnergyRating\"])\nlabels = processed['EnergyRating']\nclf = get_mlp()\npred_probs = cross_val_predict(\n    clf,\n    X_processed,\n    labels,\n    cv=kf,\n    method=\"predict_proba\",\n)\nKNN = NearestNeighbors(metric='euclidean')\nKNN.fit(X_processed.values)\n\nknn_graph = KNN.kneighbors_graph(mode=\"distance\")\ndata = {\"X\": X_processed.values, \"y\": labels}\n\nlab = Datalab(data, label_name=\"y\")\nlab.find_issues(pred_probs=pred_probs, knn_graph=knn_graph)\n\ndisplay(lab.report())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Original Data**","metadata":{}},{"cell_type":"code","source":"# Label Issues\nlabel_issues_num =  36939\noutlier_nums = 2296\nduplicates_nums = 8785\n\nissue_results = lab.get_issues(\"label\")\nsorted_issues = issue_results.sort_values(\"label_score\").index\n\nsorted_issues_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_issues, \"Unnamed: 0\"]\n})\nsave_csv_file(sorted_issues_df[:label_issues_num], f\"{output_dir}/mlp/data/label_issues.csv\")\nprint(len(sorted_issues_df))\n\nprint(\"Label Issues\")\ndisplay(X_raw.iloc[sorted_issues].assign(\n    given_label=labels.iloc[sorted_issues],\n    predicted_label=issue_results[\"predicted_label\"].iloc[sorted_issues]\n).head())\n\n# Outliers\noutlier_results = lab.get_issues(\"outlier\")\nsorted_outliers= outlier_results.sort_values(\"outlier_score\").index\nprint(\"Outliers\")\ndisplay(X_raw.iloc[sorted_outliers].head())\n\nsorted_outliers_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_outliers[:outlier_nums], \"Unnamed: 0\"]\n})\nprint(len(sorted_outliers_df))\nsave_csv_file(sorted_outliers_df, f\"{output_dir}/mlp/data/outliers.csv\")\n\n\n# Near-duplicate issues\nduplicate_results = lab.get_issues(\"near_duplicate\")\nsorted_duplicates = duplicate_results.sort_values(\"near_duplicate_score\").index\nprint(\"Near-duplicate issues\")\ndisplay(duplicate_results.sort_values(\"near_duplicate_score\").head())\nsorted_duplicates_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_duplicates, \"Unnamed: 0\"]\n})\nprint(len(sorted_duplicates_df))\nsave_csv_file(sorted_duplicates_df[:duplicates_nums], f\"{output_dir}/mlp/data/duplicates.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before Removal**","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_mlp()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    clf.fit(train_df.drop(columns=[\"EnergyRating\"]), train_df[\"EnergyRating\"])\n    acc = clf.score(test_df.drop(columns=[\"EnergyRating\"]), test_df[\"EnergyRating\"])\n    predict = clf.predict(test_df.drop(columns=[\"EnergyRating\"]))\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/mlp/data/results_before_removal.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After Removal**","metadata":{}},{"cell_type":"code","source":"for i in range(n_splits):\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    label_issue_index = pd.read_csv(f\"{output_dir}/mlp/data/label_issues.csv\")\n    train_df_removed = train_df[~train_df[\"Unnamed: 0\"].isin(label_issue_index[\"Unnamed: 0\"])]\n    print(len(train_df_removed), len(train_df_removed)/len(train_df))\n    save_csv_file(train_df_removed, f\"{output_dir}/split_{i+1}/train_removed.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/train_removed.csv\" \\\n      --output_csv_name \"processed_train_removed\" \\\n    \"\"\"\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_mlp()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train_removed.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    clf.fit(train_df.drop(columns=[\"EnergyRating\"]), train_df[\"EnergyRating\"])\n    acc = clf.score(test_df.drop(columns=[\"EnergyRating\"]), test_df[\"EnergyRating\"])\n    predict = clf.predict(test_df.drop(columns=[\"EnergyRating\"]))\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/mlp/data/results_after_removal.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scarf Embedding**","metadata":{}},{"cell_type":"code","source":"\nX_processed = pd.DataFrame(all_emb, columns=[f\"feature_{i+1}\" for i in range(all_emb.shape[1])])\nlabels = processed['EnergyRating']\nclf = get_mlp()\npred_probs = cross_val_predict(\n    clf,\n    X_processed,\n    labels,\n    cv=kf,\n    method=\"predict_proba\",\n)\nKNN = NearestNeighbors(metric='euclidean')\nKNN.fit(X_processed.values)\n\nknn_graph = KNN.kneighbors_graph(mode=\"distance\")\ndata = {\"X\": X_processed.values, \"y\": labels}\n\nlab = Datalab(data, label_name=\"y\")\nlab.find_issues(pred_probs=pred_probs, knn_graph=knn_graph)\ndisplay(lab.report())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Issues\nlabel_issues_num =  48890\noutlier_nums =  3752\nduplicates_nums = 7983\n\nissue_results = lab.get_issues(\"label\")\nsorted_issues = issue_results.sort_values(\"label_score\").index\n\nsorted_issues_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_issues, \"Unnamed: 0\"]\n})\nsave_csv_file(sorted_issues_df[:label_issues_num], f\"{output_dir}/mlp/emb/label_issues.csv\")\nprint(len(sorted_issues_df))\n\nprint(\"Label Issues\")\ndisplay(X_raw.iloc[sorted_issues].assign(\n    given_label=labels.iloc[sorted_issues],\n    predicted_label=issue_results[\"predicted_label\"].iloc[sorted_issues]\n).head())\n\n# Outliers\noutlier_results = lab.get_issues(\"outlier\")\nsorted_outliers= outlier_results.sort_values(\"outlier_score\").index\nprint(\"Outliers\")\ndisplay(X_raw.iloc[sorted_outliers].head())\n\nsorted_outliers_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_outliers[:outlier_nums], \"Unnamed: 0\"]\n})\nprint(len(sorted_outliers_df))\nsave_csv_file(sorted_outliers_df, f\"{output_dir}/mlp/emb/outliers.csv\")\n\n\n# Near-duplicate issues\nduplicate_results = lab.get_issues(\"near_duplicate\")\nsorted_duplicates = duplicate_results.sort_values(\"near_duplicate_score\").index\nprint(\"Near-duplicate issues\")\ndisplay(duplicate_results.sort_values(\"near_duplicate_score\").head())\nsorted_duplicates_df = pd.DataFrame({\n    \"Unnamed: 0\": X_raw.loc[sorted_duplicates, \"Unnamed: 0\"]\n})\nprint(len(sorted_duplicates_df))\nsave_csv_file(sorted_duplicates_df[:duplicates_nums], f\"{output_dir}/mlp/emb/duplicates.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before Removal**","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_mlp()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    train_X = np.load(f\"{output_dir}/split_{i+1}/train.npy\")\n    test_X = np.load(f\"{output_dir}/split_{i+1}/test.npy\")\n    clf.fit(train_X, train_df[\"EnergyRating\"])\n    acc = clf.score(test_X, test_df[\"EnergyRating\"],\n             )\n    predict = clf.predict(test_X)\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/mlp/emb/results_before_removal.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After Removal**","metadata":{}},{"cell_type":"code","source":"for i in range(n_splits):\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    train_X = np.load(f\"{output_dir}/split_{i+1}/train.npy\")\n    \n    label_issue_index = pd.read_csv(f\"{output_dir}/mlp/emb/label_issues.csv\")\n    train_df_removed = train_df[~train_df[\"Unnamed: 0\"].isin(label_issue_index[\"Unnamed: 0\"])]\n    train_X_removed = train_X[train_df_removed.index]\n    train_df_removed['EnergyRating'] = train_df_removed['EnergyRating'].str.strip()\n    train_df_removed['EnergyRating'] = train_df_removed['EnergyRating'].map(original_rating_encoding)\n    print(len(train_df_removed), len(train_df_removed)/len(train_df))\n    save_csv_file(train_df_removed, f\"{output_dir}/split_{i+1}/train_removed.csv\")\n    np.save(f\"{output_dir}/split_{i+1}/train_removed.npy\", train_X_removed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\n\nfor i in range(n_splits):\n    clf = get_mlp()\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/train_removed.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    train_X = np.load(f\"{output_dir}/split_{i+1}//train_removed.npy\")\n    test_X = np.load(f\"{output_dir}/split_{i+1}/test.npy\")\n    clf.fit(train_X, train_df[\"EnergyRating\"])\n    acc = clf.score(test_X, test_df[\"EnergyRating\"],\n             )\n    predict = clf.predict(test_X)\n    f1 = f1_score(test_df[\"EnergyRating\"], predict, average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")\n    results.append({\"Split\": i+1, \"Accuracy\": acc, \"F1 Score\": f1})\nresults_df = pd.DataFrame(results)\nsave_csv_file(results_df, f\"{output_dir}/mlp/emb/results_after_removal.csv\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef compare_results(data_before, data_after, emb_before, emb_after, title):\n    fig, ax = plt.subplots(2,1, figsize=(8, 8), sharex=True)\n    fig.suptitle(title)\n\n    # Plot Accuracy comparison\n    bar_width = 0.2\n    splits = data_before['Split']\n\n    ax[0].bar(splits - 1.5 * bar_width, data_before['Accuracy'], width=bar_width, label='Data Before Removal')\n    ax[0].bar(splits - 0.5 * bar_width, data_after['Accuracy'], width=bar_width, label='Data After Removal')\n    ax[0].bar(splits + 0.5 * bar_width, emb_before['Accuracy'], width=bar_width, label='Emb Before Removal')\n    ax[0].bar(splits + 1.5 * bar_width, emb_after['Accuracy'], width=bar_width, label='Emb After Removal')\n    ax[0].set_title('Accuracy Comparison')\n    ax[0].set_ylabel('Accuracy')\n\n    # Plot F1 Score comparison\n    ax[1].bar(splits - 1.5 * bar_width, data_before['F1 Score'], width=bar_width, label='Data Before Removal')\n    ax[1].bar(splits - 0.5 * bar_width, data_after['F1 Score'], width=bar_width, label='Data After Removal')\n    ax[1].bar(splits + 0.5 * bar_width, emb_before['F1 Score'], width=bar_width, label='Emb Before Removal')\n    ax[1].bar(splits + 1.5 * bar_width, emb_after['F1 Score'], width=bar_width, label='Emb After Removal')\n    ax[1].set_title('F1 Score Comparison')\n    ax[1].set_xlabel('Split')\n    ax[1].set_ylabel('F1 Score')\n\n    # plt.subplots_adjust(right=0.8)\n\n    # Create a single legend outside of the plot, on the right\n    handles, labels = ax[0].get_legend_handles_labels()\n    # fig.legend(handles, labels)\n    fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\n\n    # Display the plot\n    plt.tight_layout(rect=[0, 0, 0.85, 0.95])\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata_before = pd.read_csv(f\"{output_dir}/random_forest/data/results_before_removal.csv\")\ndata_after = pd.read_csv(f\"{output_dir}/random_forest/data/results_after_removal.csv\")\nemb_before = pd.read_csv(f\"{output_dir}/random_forest/emb/results_before_removal.csv\")\nemb_after = pd.read_csv(f\"{output_dir}/random_forest/emb/results_after_removal.csv\")\n\ncompare_results(data_before, data_after, emb_before, emb_after, \"Random Forest\")\n\ndata_before = pd.read_csv(f\"{output_dir}/mlp/data/results_before_removal.csv\")\ndata_after = pd.read_csv(f\"{output_dir}/mlp/data/results_after_removal.csv\")\nemb_before = pd.read_csv(f\"{output_dir}/mlp/emb/results_before_removal.csv\")\nemb_after = pd.read_csv(f\"{output_dir}/mlp/emb/results_after_removal.csv\")\n\ncompare_results(data_before, data_after, emb_before, emb_after, \"MLP\")\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}