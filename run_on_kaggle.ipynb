{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8580400,"sourceType":"datasetVersion","datasetId":5131411},{"sourceId":8604684,"sourceType":"datasetVersion","datasetId":5148697},{"sourceId":8606225,"sourceType":"datasetVersion","datasetId":5149720}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=182257992\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Details of Steps can be found on Google Slides via:Â¶\n\nhttps://docs.google.com/presentation/d/1sb3QkXiYooHqi3p-tkGVUqwqFKd-601_pzU96W1drw0/edit?usp=sharing","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nwandb_key_label = \"WANDB_KEY\"\nwandb_key= UserSecretsClient().get_secret(wandb_key_label)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:31:48.296661Z","iopub.execute_input":"2024-06-08T17:31:48.297123Z","iopub.status.idle":"2024-06-08T17:31:48.47862Z","shell.execute_reply.started":"2024-06-08T17:31:48.297088Z","shell.execute_reply":"2024-06-08T17:31:48.477313Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"import git\ngit.Repo.clone_from('https://github.com/Lumin-Lab/BerCuration', '/kaggle/working/scarf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r /kaggle/working/scarf/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-06-05T16:00:28.078314Z","iopub.execute_input":"2024-06-05T16:00:28.078708Z","iopub.status.idle":"2024-06-05T16:00:45.251322Z","shell.execute_reply.started":"2024-06-05T16:00:28.078677Z","shell.execute_reply":"2024-06-05T16:00:45.250169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef save_csv_file(df, path):\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n    df.to_csv(path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:27:12.478904Z","iopub.execute_input":"2024-06-08T16:27:12.479346Z","iopub.status.idle":"2024-06-08T16:27:12.486903Z","shell.execute_reply.started":"2024-06-08T16:27:12.479312Z","shell.execute_reply":"2024-06-08T16:27:12.485215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas\ndf = pd.read_csv(\"/kaggle/input/ber-stratified-samples/BER_stratified_sample.csv\")[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:43:14.352625Z","iopub.execute_input":"2024-06-08T16:43:14.35312Z","iopub.status.idle":"2024-06-08T16:43:18.679923Z","shell.execute_reply.started":"2024-06-08T16:43:14.353083Z","shell.execute_reply":"2024-06-08T16:43:18.678632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 2\noutput_dir = \"/kaggle/working/output\"\nconfig_dir=\"/kaggle/working/scarf/configs\"\nscarf_model_name = \"scarf\"\nmlp_model_name = \"mlp\"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:38:11.94177Z","iopub.execute_input":"2024-06-08T16:38:11.942281Z","iopub.status.idle":"2024-06-08T16:38:11.949066Z","shell.execute_reply.started":"2024-06-08T16:38:11.942235Z","shell.execute_reply":"2024-06-08T16:38:11.947497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport os\nkf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\nfor i, (train_index, test_index) in enumerate(kf.split(df)):\n    save_csv_file(df.iloc[train_index], f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    save_csv_file(df.iloc[test_index], f\"{output_dir}/split_{i+1}/raw_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:43:22.161012Z","iopub.execute_input":"2024-06-08T16:43:22.161501Z","iopub.status.idle":"2024-06-08T16:43:22.576761Z","shell.execute_reply.started":"2024-06-08T16:43:22.161464Z","shell.execute_reply":"2024-06-08T16:43:22.575501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/raw_train.csv\" \\\n      --output_csv_name \"processed_train\" \\\n      --is_train\n    \"\"\"\n    os.system(command)\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/raw_test.csv\" \\\n      --output_csv_name \"processed_test\"\n    \"\"\"\n    os.system(command)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:43:24.488061Z","iopub.execute_input":"2024-06-08T16:43:24.488541Z","iopub.status.idle":"2024-06-08T16:43:42.108967Z","shell.execute_reply.started":"2024-06-08T16:43:24.488503Z","shell.execute_reply":"2024-06-08T16:43:42.107735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the SCARF Encoder on the Train Dataset**","metadata":{}},{"cell_type":"code","source":"scarf_batch_size = 32\nscarf_epochs = 1\nscarf_lr = 3e-5\nscarf_emb_dim = 32\nscarf_encoder_depth = 3\nscarf_corruption_rate=0.3","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:53:11.415911Z","iopub.execute_input":"2024-06-08T16:53:11.416329Z","iopub.status.idle":"2024-06-08T16:53:11.423905Z","shell.execute_reply.started":"2024-06-08T16:53:11.416296Z","shell.execute_reply":"2024-06-08T16:53:11.422331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\"\"\"The trained scarf model is saved in \n/kaggle/working/output/split_{split}/scarf.pt if you run the following command:\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_scarf.py\\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --train_data_path=\"{output_dir}/split_{i+1}/processed_train.csv\"\\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr=3e-5 \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name=\"{scarf_model_name}\" \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --wandb_project_name='SCARF_Project' \\\n      --wandb_entity='urbancomp' \\\n      --wandb_key='{wandb_key}'\n    \"\"\"\n\n    os.system(command)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:53:28.56224Z","iopub.execute_input":"2024-06-08T16:53:28.562696Z","iopub.status.idle":"2024-06-08T16:54:27.883723Z","shell.execute_reply.started":"2024-06-08T16:53:28.56266Z","shell.execute_reply":"2024-06-08T16:54:27.881919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtain the SCARF embeddings for the Small Train dataset, and save the result**","metadata":{}},{"cell_type":"code","source":"\"\"\"The generated embeddings are saved as a NumPy array in \n/kaggle/working/output/split_{split}/train.npy if you run the following command:\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --data_path=\"{output_dir}/split_{i+1}/processed_train.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"train\"\n    \"\"\"\n\n    os.system(command)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:54:46.804723Z","iopub.execute_input":"2024-06-08T16:54:46.805176Z","iopub.status.idle":"2024-06-08T16:54:54.840774Z","shell.execute_reply.started":"2024-06-08T16:54:46.805139Z","shell.execute_reply":"2024-06-08T16:54:54.839502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtain the SCARF embeddings for the Test dataset, and save the result**","metadata":{}},{"cell_type":"code","source":"\"\"\"The generated embeddings are saved as a NumPy array in \n/kaggle/working/output/split_{split}/test.npy if you run the following command:\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --data_path=\"{output_dir}/split_{i+1}/processed_test.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"test\"\n    \"\"\"\n\n    os.system(command)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:55:16.004671Z","iopub.execute_input":"2024-06-08T16:55:16.00516Z","iopub.status.idle":"2024-06-08T16:55:24.134037Z","shell.execute_reply.started":"2024-06-08T16:55:16.005121Z","shell.execute_reply":"2024-06-08T16:55:24.132541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filter outliers based on scarf embeddings**","metadata":{}},{"cell_type":"code","source":"threshold = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:38:21.27324Z","iopub.execute_input":"2024-06-08T17:38:21.274268Z","iopub.status.idle":"2024-06-08T17:38:21.279348Z","shell.execute_reply.started":"2024-06-08T17:38:21.274225Z","shell.execute_reply":"2024-06-08T17:38:21.278153Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"from cleanlab.outlier import OutOfDistribution\nimport numpy as np\n\nfor i in range(n_splits):\n    ood = OutOfDistribution()\n    train_emb = np.load(f\"{output_dir}/split_{i+1}/train.npy\")\n    test_emb = np.load(f\"{output_dir}/split_{i+1}/test.npy\")\n    ood.fit_score(features=train_emb)\n    ood_train_feature_scores = ood.score(features=train_emb)\n    ood_test_feature_scores = ood.score(features=test_emb)\n    train_outliers_idx = np.where(ood_train_feature_scores  < threshold)[0]\n    test_outliers_idx = np.where(ood_test_feature_scores  < threshold)[0]\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_test.csv\")\n    save_csv_file(train_df.iloc[train_outliers_idx], f\"{output_dir}/split_{i+1}/cleanlab/train_outliers.csv\")\n    save_csv_file(test_df.iloc[test_outliers_idx], f\"{output_dir}/split_{i+1}/cleanlab/test_outliers.csv\")\n    save_csv_file(train_df[~train_df.index.isin(train_outliers_idx)], f\"{output_dir}/split_{i+1}/cleanlab/train_removed_outliers.csv\")\n    save_csv_file(test_df[~test_df.index.isin(test_outliers_idx)], f\"{output_dir}/split_{i+1}/cleanlab/test_removed_outliers.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:51:31.06718Z","iopub.execute_input":"2024-06-08T17:51:31.068317Z","iopub.status.idle":"2024-06-08T17:51:31.918346Z","shell.execute_reply.started":"2024-06-08T17:51:31.068247Z","shell.execute_reply":"2024-06-08T17:51:31.917176Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stdout","text":"Fitting OOD estimator based on provided features ...\nFitting OOD estimator based on provided features ...\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"Process train and test sets after their outliers are removed.\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}/cleanlab\" \\\n      --data_path \"{output_dir}/split_{i+1}/cleanlab/train_removed_outliers.csv\" \\\n      --output_csv_name \"processed_train\" \\\n      --is_train\n    \"\"\"\n    os.system(command)\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}/cleanlab\" \\\n      --data_path \"{output_dir}/split_{i+1}/cleanlab/test_removed_outliers.csv\" \\\n      --output_csv_name \"processed_test\"\n    \"\"\"\n    os.system(command)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the MLP classifier on the datasets**","metadata":{}},{"cell_type":"code","source":"mlp_batch_size = 32\nmlp_epochs = 1\nmlp_lr = 0.00003\nmlp_dropout= 0.1","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:11:53.986868Z","iopub.execute_input":"2024-06-08T18:11:53.987349Z","iopub.status.idle":"2024-06-08T18:11:53.996504Z","shell.execute_reply.started":"2024-06-08T18:11:53.98731Z","shell.execute_reply":"2024-06-08T18:11:53.99524Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"markdown","source":"**Before outlier removal**","metadata":{}},{"cell_type":"code","source":"for i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_mlp.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}/\"\\\n      --train_data_path=\"{output_dir}/split_{i+1}/processed_train.csv\" \\\n      --test_data_path=\"{output_dir}/split_{i+1}/processed_test.csv\" \\\n      --batch_size={mlp_batch_size} \\\n      --epochs={mlp_epochs} \\\n      --lr={mlp_lr} \\\n      --model_name={mlp_model_name} \\\n      --wandb_project_name \"test\" \\\n      --wandb_entity \"urbancomp\" \\\n      --wandb_key {wandb_key} \\\n      --hidden_layer 256 128 64 32 16 \\\n      --dropout={mlp_dropout} \n    \"\"\"\n\n    os.system(command)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:12:16.736384Z","iopub.execute_input":"2024-06-08T18:12:16.736892Z","iopub.status.idle":"2024-06-08T18:13:23.338588Z","shell.execute_reply.started":"2024-06-08T18:12:16.736856Z","shell.execute_reply":"2024-06-08T18:13:23.337255Z"},"trusted":true},"execution_count":168,"outputs":[{"name":"stderr","text":"wandb: Currently logged in as: dan-liu. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nwandb: Currently logged in as: dan-liu (urbancomp). Use `wandb login --relogin` to force relogin\nwandb: wandb version 0.17.1 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\nwandb: Tracking run with wandb version 0.17.0\nwandb: Run data is saved locally in /kaggle/working/wandb/run-20240608_181222-br9udic0\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run test\nwandb: â­ï¸ View project at https://wandb.ai/urbancomp/Scarf-MLP\nwandb: ðŸš€ View run at https://wandb.ai/urbancomp/Scarf-MLP/runs/br9udic0\n","output_type":"stream"},{"name":"stdout","text":"Model saved at /kaggle/working/output/split_1//mlp.pt\nEpoch [1/1] - Train Loss: 2.700, Train Acc: 0.333, Train F1: 0.167, Test Loss: 2.692, Test Acc: 0.093, Test F1: 0.011\nTest Accuracy: 0.093, Test F1: 0.011\n","output_type":"stream"},{"name":"stderr","text":"wandb: / 0.113 MB of 0.132 MB uploaded\nwandb: Run history:\nwandb:      test/A1_acc â–\nwandb:      test/A2_acc â–\nwandb:      test/A3_acc â–\nwandb:      test/B1_acc â–\nwandb:      test/B2_acc â–\nwandb:      test/B3_acc â–\nwandb:      test/C1_acc â–\nwandb:      test/C2_acc â–\nwandb:      test/C3_acc â–\nwandb:      test/D1_acc â–\nwandb:      test/D2_acc â–\nwandb:      test/E1_acc â–\nwandb:      test/E2_acc â–\nwandb:       test/F_acc â–\nwandb:       test/G_acc â–\nwandb:    test/test_acc â–\nwandb:     test/test_f1 â–\nwandb:   test/test_loss â–\nwandb:         train/f1 â–„â–â–â–…â–…â–„â–„â–‡â–…â–†â–…â–†â–†â–ˆâ–ƒ\nwandb:  train/train_acc â–ƒâ–â–â–…â–…â–ƒâ–…â–‡â–…â–†â–…â–†â–†â–ˆâ–ƒ\nwandb: train/train_loss â–‡â–‡â–ˆâ–‡â–‚â–ˆâ–ˆâ–ƒâ–…â–â–„â–â–‚â–‡â–…\nwandb: \nwandb: Run summary:\nwandb:      test/A1_acc 0.0\nwandb:      test/A2_acc 0.0\nwandb:      test/A3_acc 0.0\nwandb:      test/B1_acc 0.0\nwandb:      test/B2_acc 0.0\nwandb:      test/B3_acc 0.0\nwandb:      test/C1_acc 0.0\nwandb:      test/C2_acc 0.0\nwandb:      test/C3_acc 0.0\nwandb:      test/D1_acc 1.0\nwandb:      test/D2_acc 0.0\nwandb:      test/E1_acc 0.0\nwandb:      test/E2_acc 0.0\nwandb:       test/F_acc 0.0\nwandb:       test/G_acc 0.0\nwandb:    test/test_acc 0.09263\nwandb:     test/test_f1 0.0113\nwandb:   test/test_loss 2.69168\nwandb:         train/f1 0.00905\nwandb:  train/train_acc 0.0625\nwandb: train/train_loss 2.68456\nwandb: \nwandb: ðŸš€ View run test at: https://wandb.ai/urbancomp/Scarf-MLP/runs/br9udic0\nwandb: â­ï¸ View project at: https://wandb.ai/urbancomp/Scarf-MLP\nwandb: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240608_181222-br9udic0/logs\nwandb: Currently logged in as: dan-liu. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nwandb: Currently logged in as: dan-liu (urbancomp). Use `wandb login --relogin` to force relogin\nwandb: wandb version 0.17.1 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\nwandb: Tracking run with wandb version 0.17.0\nwandb: Run data is saved locally in /kaggle/working/wandb/run-20240608_181255-a60f8omo\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run test\nwandb: â­ï¸ View project at https://wandb.ai/urbancomp/Scarf-MLP\nwandb: ðŸš€ View run at https://wandb.ai/urbancomp/Scarf-MLP/runs/a60f8omo\n","output_type":"stream"},{"name":"stdout","text":"Model saved at /kaggle/working/output/split_2//mlp.pt\nEpoch [1/1] - Train Loss: 2.699, Train Acc: 0.000, Train F1: 0.000, Test Loss: 2.686, Test Acc: 0.095, Test F1: 0.012\nTest Accuracy: 0.095, Test F1: 0.012\n","output_type":"stream"},{"name":"stderr","text":"wandb: / 0.114 MB of 0.133 MB uploaded\nwandb: Run history:\nwandb:      test/A1_acc â–\nwandb:      test/A2_acc â–\nwandb:      test/A3_acc â–\nwandb:      test/B1_acc â–\nwandb:      test/B2_acc â–\nwandb:      test/B3_acc â–\nwandb:      test/C1_acc â–\nwandb:      test/C2_acc â–\nwandb:      test/C3_acc â–\nwandb:      test/D1_acc â–\nwandb:      test/D2_acc â–\nwandb:      test/E1_acc â–\nwandb:      test/E2_acc â–\nwandb:       test/F_acc â–\nwandb:       test/G_acc â–\nwandb:    test/test_acc â–\nwandb:     test/test_f1 â–\nwandb:   test/test_loss â–\nwandb:         train/f1 â–…â–â–‚â–â–…â–„â–„â–‚â–‚â–ƒâ–ƒâ–‚â–‡â–ˆâ–‚\nwandb:  train/train_acc â–…â–â–‚â–â–…â–ƒâ–†â–‚â–‚â–ƒâ–ƒâ–‚â–ˆâ–‡â–‚\nwandb: train/train_loss â–ƒâ–…â–„â–†â–ƒâ–‚â–„â–„â–…â–ƒâ–„â–ˆâ–â–„â–…\nwandb: \nwandb: Run summary:\nwandb:      test/A1_acc 0.0\nwandb:      test/A2_acc 0.0\nwandb:      test/A3_acc 0.0\nwandb:      test/B1_acc 0.0\nwandb:      test/B2_acc 0.0\nwandb:      test/B3_acc 0.0\nwandb:      test/C1_acc 0.0\nwandb:      test/C2_acc 0.0\nwandb:      test/C3_acc 0.0\nwandb:      test/D1_acc 1.0\nwandb:      test/D2_acc 0.0\nwandb:      test/E1_acc 0.0\nwandb:      test/E2_acc 0.0\nwandb:       test/F_acc 0.0\nwandb:       test/G_acc 0.0\nwandb:    test/test_acc 0.09462\nwandb:     test/test_f1 0.01153\nwandb:   test/test_loss 2.68615\nwandb:         train/f1 0.0098\nwandb:  train/train_acc 0.0625\nwandb: train/train_loss 2.70967\nwandb: \nwandb: ðŸš€ View run test at: https://wandb.ai/urbancomp/Scarf-MLP/runs/a60f8omo\nwandb: â­ï¸ View project at: https://wandb.ai/urbancomp/Scarf-MLP\nwandb: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240608_181255-a60f8omo/logs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**After outlier removal**","metadata":{}},{"cell_type":"code","source":"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_mlp.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}/cleanlab\"\\\n      --train_data_path=\"{output_dir}/split_{i+1}/cleanlab/processed_train.csv\" \\\n      --test_data_path=\"{output_dir}/split_{i+1}/cleanlab/processed_test.csv\" \\\n      --batch_size={mlp_batch_size} \\\n      --epochs={mlp_epochs} \\\n      --lr={mlp_lr} \\\n      --model_name={mlp_model_name} \\\n      --wandb_project_name \"test\" \\\n      --wandb_entity \"urbancomp\" \\\n      --wandb_key {wandb_key} \\\n      --hidden_layer 256 128 64 32 16 \\\n      --dropout={mlp_dropout} \n    \"\"\"\n\n    os.system(command)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:13:23.340798Z","iopub.execute_input":"2024-06-08T18:13:23.341279Z","iopub.status.idle":"2024-06-08T18:14:29.553887Z","shell.execute_reply.started":"2024-06-08T18:13:23.341235Z","shell.execute_reply":"2024-06-08T18:14:29.552526Z"},"trusted":true},"execution_count":169,"outputs":[{"name":"stderr","text":"wandb: Currently logged in as: dan-liu. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nwandb: Currently logged in as: dan-liu (urbancomp). Use `wandb login --relogin` to force relogin\nwandb: wandb version 0.17.1 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\nwandb: Tracking run with wandb version 0.17.0\nwandb: Run data is saved locally in /kaggle/working/wandb/run-20240608_181328-u6socy7m\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run test\nwandb: â­ï¸ View project at https://wandb.ai/urbancomp/Scarf-MLP\nwandb: ðŸš€ View run at https://wandb.ai/urbancomp/Scarf-MLP/runs/u6socy7m\n","output_type":"stream"},{"name":"stdout","text":"Model saved at /kaggle/working/output/split_1/cleanlab/mlp.pt\nEpoch [1/1] - Train Loss: 2.700, Train Acc: 0.333, Train F1: 0.167, Test Loss: 2.692, Test Acc: 0.093, Test F1: 0.011\nTest Accuracy: 0.093, Test F1: 0.011\n","output_type":"stream"},{"name":"stderr","text":"wandb: / 0.113 MB of 0.132 MB uploaded\nwandb: Run history:\nwandb:      test/A1_acc â–\nwandb:      test/A2_acc â–\nwandb:      test/A3_acc â–\nwandb:      test/B1_acc â–\nwandb:      test/B2_acc â–\nwandb:      test/B3_acc â–\nwandb:      test/C1_acc â–\nwandb:      test/C2_acc â–\nwandb:      test/C3_acc â–\nwandb:      test/D1_acc â–\nwandb:      test/D2_acc â–\nwandb:      test/E1_acc â–\nwandb:      test/E2_acc â–\nwandb:       test/F_acc â–\nwandb:       test/G_acc â–\nwandb:    test/test_acc â–\nwandb:     test/test_f1 â–\nwandb:   test/test_loss â–\nwandb:         train/f1 â–„â–â–â–…â–…â–„â–„â–‡â–…â–†â–…â–†â–†â–ˆâ–ƒ\nwandb:  train/train_acc â–ƒâ–â–â–…â–…â–ƒâ–…â–‡â–…â–†â–…â–†â–†â–ˆâ–ƒ\nwandb: train/train_loss â–‡â–‡â–ˆâ–‡â–‚â–ˆâ–ˆâ–ƒâ–…â–â–„â–â–‚â–‡â–…\nwandb: \nwandb: Run summary:\nwandb:      test/A1_acc 0.0\nwandb:      test/A2_acc 0.0\nwandb:      test/A3_acc 0.0\nwandb:      test/B1_acc 0.0\nwandb:      test/B2_acc 0.0\nwandb:      test/B3_acc 0.0\nwandb:      test/C1_acc 0.0\nwandb:      test/C2_acc 0.0\nwandb:      test/C3_acc 0.0\nwandb:      test/D1_acc 1.0\nwandb:      test/D2_acc 0.0\nwandb:      test/E1_acc 0.0\nwandb:      test/E2_acc 0.0\nwandb:       test/F_acc 0.0\nwandb:       test/G_acc 0.0\nwandb:    test/test_acc 0.09263\nwandb:     test/test_f1 0.0113\nwandb:   test/test_loss 2.69168\nwandb:         train/f1 0.00905\nwandb:  train/train_acc 0.0625\nwandb: train/train_loss 2.68456\nwandb: \nwandb: ðŸš€ View run test at: https://wandb.ai/urbancomp/Scarf-MLP/runs/u6socy7m\nwandb: â­ï¸ View project at: https://wandb.ai/urbancomp/Scarf-MLP\nwandb: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240608_181328-u6socy7m/logs\nwandb: Currently logged in as: dan-liu. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nwandb: Currently logged in as: dan-liu (urbancomp). Use `wandb login --relogin` to force relogin\nwandb: wandb version 0.17.1 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\nwandb: Tracking run with wandb version 0.17.0\nwandb: Run data is saved locally in /kaggle/working/wandb/run-20240608_181401-2syzcy9y\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run test\nwandb: â­ï¸ View project at https://wandb.ai/urbancomp/Scarf-MLP\nwandb: ðŸš€ View run at https://wandb.ai/urbancomp/Scarf-MLP/runs/2syzcy9y\n","output_type":"stream"},{"name":"stdout","text":"Model saved at /kaggle/working/output/split_2/cleanlab/mlp.pt\nEpoch [1/1] - Train Loss: 2.699, Train Acc: 0.000, Train F1: 0.000, Test Loss: 2.686, Test Acc: 0.095, Test F1: 0.012\nTest Accuracy: 0.095, Test F1: 0.012\n","output_type":"stream"},{"name":"stderr","text":"wandb: / 0.114 MB of 0.133 MB uploaded\nwandb: Run history:\nwandb:      test/A1_acc â–\nwandb:      test/A2_acc â–\nwandb:      test/A3_acc â–\nwandb:      test/B1_acc â–\nwandb:      test/B2_acc â–\nwandb:      test/B3_acc â–\nwandb:      test/C1_acc â–\nwandb:      test/C2_acc â–\nwandb:      test/C3_acc â–\nwandb:      test/D1_acc â–\nwandb:      test/D2_acc â–\nwandb:      test/E1_acc â–\nwandb:      test/E2_acc â–\nwandb:       test/F_acc â–\nwandb:       test/G_acc â–\nwandb:    test/test_acc â–\nwandb:     test/test_f1 â–\nwandb:   test/test_loss â–\nwandb:         train/f1 â–…â–â–‚â–â–…â–„â–„â–‚â–‚â–ƒâ–ƒâ–‚â–‡â–ˆâ–‚\nwandb:  train/train_acc â–…â–â–‚â–â–…â–ƒâ–†â–‚â–‚â–ƒâ–ƒâ–‚â–ˆâ–‡â–‚\nwandb: train/train_loss â–ƒâ–…â–„â–†â–ƒâ–‚â–„â–„â–…â–ƒâ–„â–ˆâ–â–„â–…\nwandb: \nwandb: Run summary:\nwandb:      test/A1_acc 0.0\nwandb:      test/A2_acc 0.0\nwandb:      test/A3_acc 0.0\nwandb:      test/B1_acc 0.0\nwandb:      test/B2_acc 0.0\nwandb:      test/B3_acc 0.0\nwandb:      test/C1_acc 0.0\nwandb:      test/C2_acc 0.0\nwandb:      test/C3_acc 0.0\nwandb:      test/D1_acc 1.0\nwandb:      test/D2_acc 0.0\nwandb:      test/E1_acc 0.0\nwandb:      test/E2_acc 0.0\nwandb:       test/F_acc 0.0\nwandb:       test/G_acc 0.0\nwandb:    test/test_acc 0.09462\nwandb:     test/test_f1 0.01153\nwandb:   test/test_loss 2.68615\nwandb:         train/f1 0.0098\nwandb:  train/train_acc 0.0625\nwandb: train/train_loss 2.70967\nwandb: \nwandb: ðŸš€ View run test at: https://wandb.ai/urbancomp/Scarf-MLP/runs/2syzcy9y\nwandb: â­ï¸ View project at: https://wandb.ai/urbancomp/Scarf-MLP\nwandb: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240608_181401-2syzcy9y/logs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Train the random forest on the datasets**","metadata":{}}]}