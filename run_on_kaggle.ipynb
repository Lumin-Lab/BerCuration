{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8580400,"sourceType":"datasetVersion","datasetId":5131411},{"sourceId":8604684,"sourceType":"datasetVersion","datasetId":5148697},{"sourceId":8606225,"sourceType":"datasetVersion","datasetId":5149720}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=182267883\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/luminlab/ber-curation?scriptVersionId=182257992\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"Details of Steps can be found on Google Slides via:¶\n\nhttps://docs.google.com/presentation/d/1sb3QkXiYooHqi3p-tkGVUqwqFKd-601_pzU96W1drw0/edit?usp=sharing","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nwandb_key_label = \"WANDB_KEY\"\nwandb_key= UserSecretsClient().get_secret(wandb_key_label)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:55:13.355629Z","iopub.execute_input":"2024-06-08T19:55:13.356221Z","iopub.status.idle":"2024-06-08T19:55:13.549362Z","shell.execute_reply.started":"2024-06-08T19:55:13.356193Z","shell.execute_reply":"2024-06-08T19:55:13.548468Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import git\ngit.Repo.clone_from('https://github.com/Lumin-Lab/BerCuration', '/kaggle/working/scarf')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:55:15.204829Z","iopub.execute_input":"2024-06-08T19:55:15.205416Z","iopub.status.idle":"2024-06-08T19:55:15.827428Z","shell.execute_reply.started":"2024-06-08T19:55:15.205386Z","shell.execute_reply":"2024-06-08T19:55:15.826516Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<git.repo.base.Repo '/kaggle/working/scarf/.git'>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -r /kaggle/working/scarf/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:55:18.668137Z","iopub.execute_input":"2024-06-08T19:55:18.668773Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting torch==2.3.0 (from -r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting pandas==2.2.2 (from -r /kaggle/working/scarf/requirements.txt (line 2))\n  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: wandb==0.17.0 in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/scarf/requirements.txt (line 3)) (0.17.0)\nCollecting scikit-learn==1.5.0 (from -r /kaggle/working/scarf/requirements.txt (line 4))\n  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting cleanlab==2.6.5 (from -r /kaggle/working/scarf/requirements.txt (line 5))\n  Downloading cleanlab-2.6.5-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting matplotlib==3.9.0 (from -r /kaggle/working/scarf/requirements.txt (line 6))\n  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting python-dotenv==1.0.1 (from -r /kaggle/working/scarf/requirements.txt (line 7))\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2->-r /kaggle/working/scarf/requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2->-r /kaggle/working/scarf/requirements.txt (line 2)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2->-r /kaggle/working/scarf/requirements.txt (line 2)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2->-r /kaggle/working/scarf/requirements.txt (line 2)) (2023.4)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (4.2.2)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (69.0.3)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.0->-r /kaggle/working/scarf/requirements.txt (line 4)) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.0->-r /kaggle/working/scarf/requirements.txt (line 4)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.0->-r /kaggle/working/scarf/requirements.txt (line 4)) (3.2.0)\nRequirement already satisfied: tqdm>=4.53.0 in /opt/conda/lib/python3.10/site-packages (from cleanlab==2.6.5->-r /kaggle/working/scarf/requirements.txt (line 5)) (4.66.4)\nRequirement already satisfied: termcolor>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from cleanlab==2.6.5->-r /kaggle/working/scarf/requirements.txt (line 5)) (2.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (21.3)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.0->-r /kaggle/working/scarf/requirements.txt (line 6)) (3.1.1)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1))\n  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0->-r /kaggle/working/scarf/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.17.0->-r /kaggle/working/scarf/requirements.txt (line 3)) (5.0.1)\nDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cleanlab-2.6.5-py3-none-any.whl (352 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.3/352.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: triton, python-dotenv, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, scikit-learn, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, nvidia-cusolver-cu12, cleanlab, torch\n  Attempting uninstall: python-dotenv\n    Found existing installation: python-dotenv 1.0.0\n    Uninstalling python-dotenv-1.0.0:\n      Successfully uninstalled python-dotenv-1.0.0\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.1\n    Uninstalling pandas-2.2.1:\n      Successfully uninstalled pandas-2.2.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\ndef save_csv_file(df, path):\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n    df.to_csv(path, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas\ndf = pd.read_csv(\"/kaggle/input/ber-stratified-samples/BER_stratified_sample.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 5\noutput_dir = \"/kaggle/working/output\"\nconfig_dir=\"/kaggle/working/scarf/configs\"\nscarf_model_name = \"scarf\"\nmlp_model_name = \"mlp\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport os\nkf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\nfor i, (train_index, test_index) in enumerate(kf.split(df)):\n    save_csv_file(df.iloc[train_index], f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    save_csv_file(df.iloc[test_index], f\"{output_dir}/split_{i+1}/raw_test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/raw_train.csv\" \\\n      --output_csv_name \"processed_train\" \\\n      --is_train\n    \"\"\"\n    os.system(command)\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}\" \\\n      --data_path \"{output_dir}/split_{i+1}/raw_test.csv\" \\\n      --output_csv_name \"processed_test\"\n    \"\"\"\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the SCARF Encoder on the Train Dataset**","metadata":{}},{"cell_type":"code","source":"scarf_batch_size = 32\nscarf_epochs = 25\nscarf_lr = 3e-5\nscarf_emb_dim = 32\nscarf_encoder_depth = 3\nscarf_corruption_rate=0.3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\"\"\"The trained scarf model is saved in \n/kaggle/working/output/split_{split}/scarf.pt if you run the following command:\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_scarf.py\\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --train_data_path=\"{output_dir}/split_{i+1}/processed_train.csv\"\\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name=\"{scarf_model_name}\" \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --wandb_project_name='SCARF_Project' \\\n      --wandb_entity='urbancomp' \\\n      --wandb_key='{wandb_key}'\n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtain the SCARF embeddings for different train splits, and save the result**","metadata":{}},{"cell_type":"code","source":"\"\"\"The generated embeddings are saved as a NumPy array in \n/kaggle/working/output/split_{split}/train.npy if you run the following command:\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --data_path=\"{output_dir}/split_{i+1}/processed_train.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"train\"\n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtain the SCARF embeddings for different test splits, and save the result**","metadata":{}},{"cell_type":"code","source":"\"\"\"The generated embeddings are saved as a NumPy array in \n/kaggle/working/output/split_{split}/test.npy if you run the following command:\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_scarf_embedding.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}\" \\\n      --data_path=\"{output_dir}/split_{i+1}/processed_test.csv\" \\\n      --batch_size={scarf_batch_size} \\\n      --epochs={scarf_epochs} \\\n      --lr={scarf_lr} \\\n      --emb_dim={scarf_emb_dim} \\\n      --encoder_depth={scarf_encoder_depth} \\\n      --model_name={scarf_model_name} \\\n      --corruption_rate={scarf_corruption_rate} \\\n      --embedding_save_name=\"test\"\n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filter outliers based on scarf embeddings**","metadata":{}},{"cell_type":"code","source":"threshold = 0.3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map = {\n    0: 'A1',\n    1: 'A2',\n    2: 'A3',\n    3: 'B1',\n    4: 'B2',\n    5: 'B3',\n    6: 'C1',\n    7: 'C2',\n    8: 'C3',\n    9: 'D1',\n    10: 'D2',\n    11: 'E1',\n    12: 'E2',\n    13: 'F',\n    14: 'G'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_data(data, labels, circles, title, alpha=1.0):\n    \n    colormap = plt.cm.tab20\n    colors = {str(label): colormap(i) for i, label in enumerate(set(labels))}\n\n    plt.figure(figsize=(14, 5))\n    done = set()\n    for i in range(0,len(data)):\n        lab = str(labels[i])\n        label_name = label_map[int(lab)]\n        if label_name in done:\n            label = \"\"\n        else:\n            label = label_name\n            done.add(label_name)\n        plt.scatter(data[i, 0], data[i, 1],  c=colors[lab], s=30,alpha=0.6, \n                    label = label)\n    for i in circles:\n        plt.plot(\n            data[i][0],\n            data[i][1],\n            \"o\",\n            markerfacecolor=\"none\",\n            markeredgecolor=\"red\",\n            markersize=14,\n            markeredgewidth=2.5,\n            alpha=alpha\n        )\n    _ = plt.title(title, fontsize=25)\n    plt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cleanlab.outlier import OutOfDistribution\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nfor i in range(n_splits):\n    ood = OutOfDistribution()\n    train_emb = np.load(f\"{output_dir}/split_{i+1}/train.npy\")\n    test_emb = np.load(f\"{output_dir}/split_{i+1}/test.npy\")\n    ood.fit_score(features=train_emb)\n    ood_train_feature_scores = ood.score(features=train_emb)\n    ood_test_feature_scores = ood.score(features=test_emb)\n    train_outliers_idx = np.where(ood_train_feature_scores  < threshold)[0]\n    test_outliers_idx = np.where(ood_test_feature_scores  < threshold)[0]\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/raw_test.csv\")\n    save_csv_file(train_df.iloc[train_outliers_idx], f\"{output_dir}/split_{i+1}/cleanlab/train_outliers.csv\")\n    save_csv_file(test_df.iloc[test_outliers_idx], f\"{output_dir}/split_{i+1}/cleanlab/test_outliers.csv\")\n    save_csv_file(train_df[~train_df.index.isin(train_outliers_idx)], f\"{output_dir}/split_{i+1}/cleanlab/train_removed_outliers.csv\")\n    save_csv_file(test_df[~test_df.index.isin(test_outliers_idx)], f\"{output_dir}/split_{i+1}/cleanlab/test_removed_outliers.csv\")\n    \n    pca = PCA(n_components=2)\n    process_train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train.csv\")\n    \n    process_test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_test.csv\")\n    \n    pca.fit(process_train_df.drop(columns=[\"EnergyRating\"]))\n    low_dim_train_features = pca.transform(process_train_df.drop(columns=[\"EnergyRating\"]))\n    low_dim_test_features = pca.transform(process_test_df.drop(columns=[\"EnergyRating\"]))\n    \n    colormap = plt.cm.tab20\n    colors = [colormap(i) for i in range(len(label_map))]\n    \n    plot_data(low_dim_train_features, process_train_df[\"EnergyRating\"], \n              train_outliers_idx, \n              title = f\"Split {i+1}: Outliers in Training Set\",\n             )\n    plot_data(low_dim_test_features, process_test_df[\"EnergyRating\"], \n              test_outliers_idx, \n              title = f\"Split {i+1}: Outliers in Test Set\",\n             )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Process train and test sets after their outliers are removed.\n\"\"\"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}/cleanlab\" \\\n      --data_path \"{output_dir}/split_{i+1}/cleanlab/train_removed_outliers.csv\" \\\n      --output_csv_name \"processed_train\" \\\n      --is_train\n    \"\"\"\n    os.system(command)\n    command = f\"\"\"\n    python /kaggle/working/scarf/get_processed_dataset.py \\\n      --config_dir \"/kaggle/working/scarf/configs\" \\\n      --output_dir \"{output_dir}/split_{i+1}/cleanlab\" \\\n      --data_path \"{output_dir}/split_{i+1}/cleanlab/test_removed_outliers.csv\" \\\n      --output_csv_name \"processed_test\"\n    \"\"\"\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the MLP classifier on the datasets**","metadata":{}},{"cell_type":"code","source":"mlp_batch_size = 32\nmlp_epochs = 25\nmlp_lr = 0.00003\nmlp_dropout= 0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before outlier removal**","metadata":{}},{"cell_type":"code","source":"for i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_mlp.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}/\"\\\n      --train_data_path=\"{output_dir}/split_{i+1}/processed_train.csv\" \\\n      --test_data_path=\"{output_dir}/split_{i+1}/cleanlab/processed_test.csv\" \\\n      --batch_size={mlp_batch_size} \\\n      --epochs={mlp_epochs} \\\n      --lr={mlp_lr} \\\n      --model_name={mlp_model_name} \\\n      --wandb_project_name \"test\" \\\n      --wandb_entity \"urbancomp\" \\\n      --wandb_key {wandb_key} \\\n      --hidden_layer 256 128 64 32 16 \\\n      --dropout={mlp_dropout} \n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After outlier removal**","metadata":{}},{"cell_type":"code","source":"\nfor i in range(n_splits):\n    command = f\"\"\"\n    python /kaggle/working/scarf/run_mlp.py \\\n      --config_dir={config_dir} \\\n      --output_dir=\"{output_dir}/split_{i+1}/cleanlab\"\\\n      --train_data_path=\"{output_dir}/split_{i+1}/cleanlab/processed_train.csv\" \\\n      --test_data_path=\"{output_dir}/split_{i+1}/cleanlab/processed_test.csv\" \\\n      --batch_size={mlp_batch_size} \\\n      --epochs={mlp_epochs} \\\n      --lr={mlp_lr} \\\n      --model_name={mlp_model_name} \\\n      --wandb_project_name \"test\" \\\n      --wandb_entity \"urbancomp\" \\\n      --wandb_key {wandb_key} \\\n      --hidden_layer 256 128 64 32 16 \\\n      --dropout={mlp_dropout} \n    \"\"\"\n\n    os.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the random forest on the datasets**","metadata":{}},{"cell_type":"code","source":"max_depth = 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before outlier removal**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfor i in range(n_splits):\n    clf = RandomForestClassifier(max_depth=max_depth, random_state=42)\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/processed_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/cleanlab/processed_test.csv\")\n    clf.fit(train_df.drop(columns=[\"EnergyRating\"]), train_df[\"EnergyRating\"])\n    acc = clf.score(test_df.drop(columns=[\"EnergyRating\"]), test_df[\"EnergyRating\"],\n             )\n    predict = clf.predict(test_df.drop(columns=[\"EnergyRating\"]))\n    f1 = f1_score(test_df[\"EnergyRating\"], predict,average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After outlier removal**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfor i in range(n_splits):\n    clf = RandomForestClassifier(max_depth=max_depth, random_state=42)\n    train_df = pd.read_csv(f\"{output_dir}/split_{i+1}/cleanlab/processed_train.csv\")\n    test_df = pd.read_csv(f\"{output_dir}/split_{i+1}/cleanlab/processed_test.csv\")\n    clf.fit(train_df.drop(columns=[\"EnergyRating\"]), train_df[\"EnergyRating\"])\n    acc = clf.score(test_df.drop(columns=[\"EnergyRating\"]), test_df[\"EnergyRating\"],\n             )\n    predict = clf.predict(test_df.drop(columns=[\"EnergyRating\"]))\n    f1 = f1_score(test_df[\"EnergyRating\"], predict,average='macro')\n    print(f\"Split: {i+1}, Accuracy: {acc}, F1 Score: {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}